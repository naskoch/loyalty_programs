\input{\texfolder/cust_behv.tex}

\subsection{Merchant Objective Dynamics}
We substitute the value of the phase transition point obtained above in the rate of revenue equations to reevaluate them. 
And since we assume that $\lambda$ and $t$ are drawn independent of each other, we can separate the expectation terms and evaluate them sequentially, first over $t$, then over $\lambda$. This reduces the rate of revenues as follows:

\begin{align*}
RoR_A =& \underset{\lambda, t}E\left[\frac{k-R}{i_0(t)/\lambda + k - i_0(t)}\right]\\
                                       =& \underset{\lambda}E\left[p\cdot\frac{k-R}{i_0/\lambda + k - i_0} + (1-p)\frac{\lambda(k-R)}{k}\right]\\
                                       =& \underset{\lambda}E\left[p\cdot\frac{\lambda(k-R)}{k\lambda + i_0(1-\lambda)} + (1-p)\frac{\lambda(k-R)}{k}\right]\\
                                       =& p\cdot\frac{k-R}{b(k-i_0)^2}\cdot\left(b(k-i_0) - i_0 \log\left(1 + \frac{b(k-i_0)}{i_0}\right)\right) + (1-p)\frac{b(k-R)}{2k}\\
                                       =& p\cdot\frac{k-R}{b\Delta^2}\cdot\left(b\Delta - (k-\Delta)\log\left(1+\frac{b\Delta}{k-\Delta}\right)\right) + (1-p)\frac{b(k-R)}{2k}\\
                                       =& p\cdot\frac{k-R}{\Delta}\cdot\left(1 - \frac{k-\Delta}{b\Delta}\log\left(1+\frac{b\Delta}{k-\Delta}\right)\right) + (1-p)\frac{b(k-R)}{2k}
\end{align*}

\begin{align*}
RoR_B =& \underset{\lambda, t}E\left[\frac{(i_0(t)\lambda - i_0(t))(1-v)}{i_0(t)/\lambda + k - i_0(t)}\right]\\
                                     =& \underset{\lambda}E\left[p\cdot\frac{(i_0/\lambda - i_0)(1-v)}{i_0/\lambda + k - i_0} + (1-p)\frac{(k/\lambda - k)(1-v)}{k/\lambda}\right]\\
                                     =& \underset{\lambda}E\left[p\cdot\frac{i_0(1-\lambda)(1-v)}{k\lambda + i_0(1-\lambda)} + (1-p)(1-\lambda)(1-v)\right]\\
                                     =& p\cdot\frac{i_0(1-v)}{b(k-i_0)^2}\left(k\log\left(1+\frac{b(k-i_0)}{i_0}\right) - b(k-i_0)\right) + (1-p)(1-\frac{b}{2})(1-v)\\
                                     =& p\cdot\frac{(k-\Delta)(1-v)}{b\Delta^2}\left(k\log\left(1+\frac{b\Delta}{k-\Delta}\right) - b\Delta\right) + (1-p)(1-\frac{b}{2})(1-v)\\
                                     =& p\cdot\frac{(k-\Delta)(1-v)}{\Delta}\left(\frac{k}{b\Delta}\log\left(1+\frac{b\Delta}{k-\Delta}\right) - 1\right) + (1-p)(1-\frac{b}{2})(1-v)\label{eq:rorB_eval}
\end{align*}

The above framework can be used for optimizing for the reward parameters to maximize $A$'s rate of revenue, and correspondingly for the discount parameter for $B$, for varying distributions of the customer population.
We leave the competitive study where merchant $B$ could strategize on its discount value $v$ for future work.
And in this work we only consider proportional promotion budgeting as mentioned in Section~\ref{sec:model}, where $R = \alpha k v$ and $\alpha$ is a constant.

\begin{theorem}
Under proportional promotion budgeting, the optimal reward distance that $A$ should set is $k = \frac{e}{\alpha(1-\beta)}$ at all values of $b$ as long as $\beta$ is close to 1.
\end{theorem}
\proof
Recall the previous expression for $RoR_A$. Maximizing this function is equivalent to maximizing the following:
\begin{align*}
\underset{k}\max\{RoR_A\} \Leftrightarrow & \underset{k} \max\left\{\frac{k}{\Delta}\left(1-\frac{k-\Delta}{b\Delta}\log\left(\frac{k-\Delta(1-b)}{k-\Delta}\right)\right)\right\}
\end{align*}
Now let $\theta = \frac{k}{\Delta}$. Then maximizing the above function is equivalent to maximizing the following function w.r.t. $\theta$. Note that $\theta \geq 1$ because $k \geq \Delta$.
\beq\notag
\underset{k}\max\{RoR_A\} \Leftrightarrow \underset{\theta}\max\{f(\theta)\} \Leftrightarrow \underset{\theta}\max\left\{ \theta \left(1-\frac{\theta-1}{b}\log\left(1 + \frac{b}{\theta-1}\right)\right)\right\} 
\eeq

We will show that $f'(\theta) \leq 0$ for all $\theta$ so maximizing $f$ is equivalent to minimizing $\theta$.

\begin{align*}
f'(\theta) &= \frac{2\theta-1+b}{\theta-1+b} - \frac{2\theta-1}{b} \log \left(1+\frac{b}{\theta-1} \right) \\
&= \frac{2\theta-1}{b}\cdot\left(\left(\frac{2\theta-1+b}{\theta-1+b}\right)\left(\frac{b}{2\theta-1} \right) - \log \left(1+\frac{b}{\theta-1} \right)\right) \\
&= \frac{2\theta-1}{b}\cdot\left(\frac{b}{\theta-1+b}+\frac{b^2}{(2\theta-1)(\theta-1+b)} - \log \left(1+\frac{b}{\theta-1} \right)\right)
\end{align*}

Let $g(b, \theta) = \frac{b}{\theta-1+b}+\frac{b^2}{(2\theta-1)(\theta-1+b)} - \log \left(1+\frac{b}{\theta-1} \right)$. 
In the limit of $b \to 0$, it is easy to see that $f'(\theta) = g(b, \theta) = 0$ for all $\theta$ in the domain. 
We now show that for all $\theta$ in the domain and all $0 < b \leq 1$, $\frac{\partial g(b, \theta)}{\partial b} \leq 0$.

\begin{align*}
\frac{\partial g(b,\theta)}{\partial b} &= \frac{\theta-1}{(\theta-1+b)^2}+\frac{1}{2\theta-1}\cdot \frac{2b(\theta-1+b)-b^2}{(\theta-1+b)^2} -\frac{1}{\theta-1+b} \leq 0 \\
&\iff \theta-1 + \frac{2b(\theta-1+b)-b^2}{2\theta-1} \leq \theta-1+b \\
&\iff \frac{b(2\theta-2+b)}{2\theta-1} \leq b \\
&\iff 2\theta-2+b \leq 2\theta-1 \\
&\iff b \leq 1
\end{align*}

Thus we have shown that for all $\theta$, $g(b,\theta) = 0$ as $b \to 0$ and that for all $\theta$ and $0 < b \leq 1$, $g(b,\theta)$ is decreasing. 
These together mean that for all $\theta$ and $b \in [0,1]$, $g(b,\theta) \leq 0$. 
Which implies that $f'(\theta) \le 0$ for all $\theta$ in the domain.
So to maximize $f$, we need to minimize $\theta$.

Observe that minimizing $\theta = \frac{k}{\Delta}$ is equivalent to minimizing the influence zone.
As shown in Remark~\ref{rem:inf_zone}, this happens at $k = \frac{e}{\alpha(1-\beta)}$. 
\endproof

An interesting point to observe above is that maximizing the revenue objective is equivalent to minimizing the influence zone.
This result matches the following intuition - the faster the merchant can get customers to adopt the reward program, the more transactions it will make in the long run - but is stronger as it actually maximizes the revenue objective as well. 
Although, reward point accelerations are common and effective mechanisms to get customers to adopt reward programs, we have shown that designing the reward program so that minimum number of such accelerations are required, leads to maximizing merchant's revenue. 
The condition that $\beta$ be close to 1 is not very restrictive, as the discount factor is expected to be high in most cases. 
Note that because $k \geq \Delta$, the above also shows $\alpha \leq e$. 
Finally, observe that we need $R > \frac{(1-\lambda)v}{1-\beta}$ for $V$ to be increasing. 
We meet this condition with proportional budgeting when $k = \frac{e}{\alpha(1-\beta)}$ as $R = \alpha k v = \frac{e v}{1-\beta} \geq \frac{v}{1-\beta} \geq \frac{(1-\lambda)v}{1-\beta}$. 

Now we characterize conditions for when it is strictly better for $A$ to offer a reward program.
This boils down to two situations: first, the rate of revenue for $A$ should be higher than that of $B$ and second, that the rate of revenue for $A$ should be higher than it could have achieved by not offering the reward program.
First we evaluate the expected rate of revenues for both $A$ and $B$ under these conditions of proportional budgeting and optimal $k$.

\begin{align*}
RoR_A =& pk\cdot\frac{1-\alpha v}{\Delta}\cdot\left(1 - \frac{k-\Delta}{b\Delta}\log\left(1+\frac{b\Delta}{k-\Delta}\right)\right) + (1-p)\frac{bk(1-\alpha v)}{2k}\\
      =& (1-\alpha v) \left(p\frac{e}{\alpha}\left(1-\frac{e-\alpha}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right)\right) + (1-p)\frac{b}{2}\right)
\end{align*}

\begin{align*}
RoR_B =& p\cdot\frac{(k-\Delta)(1-v)}{\Delta}\left(\frac{k}{b\Delta}\log\left(1+\frac{b\Delta}{k-\Delta}\right) - 1\right) + (1-p)(1-\frac{b}{2})(1-v)\\
      =& (1-v)\left(p\cdot\frac{e-\alpha}{\alpha}\left(\frac{e}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right) - 1\right)\right)\\
      =& (1-v)\left(p\frac{e}{\alpha}\left(\frac{e-\alpha}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right) - \frac{e-\alpha}{e}\right) + (1-p)(1-\frac{b}{2})\right)
\end{align*}

Observe that both the above equations have a left term and a right term. The left term is the rate of revenue obtained from forward-looking customers whereas the right term is from the myopic customers.
As $\alpha$ ranges between $0$ and $e$, the value on the left term increases from $0$ for $RoR_A$ and decreases to $0$ for $RoR_B$.
That is by controlling the reward budget ratio, $A$ is able to gain the entire forward-looking customer base.
But now observe how $RoR_A$ varies with $\alpha$.
The marginal revenue term $(1-\alpha v)$ decreases with $\alpha$ as the merchant is giving higher rewards to customers, whereas the market share term increases as $A$ gains more forward-looking customers with increase in reward budget.
As $\alpha \to 0$, $RoR_A \to b/2$, \ie, the revenue earned is only due to visit probability bias, and is equivalent to the reveue earned by $A$ when not running any reward program.

Figure~\ref{fig:offer_reward_or_not} illustrates the region in terms of the customer parameters $(b,p)$ where it is better for $A$ to offer a reward program, \ie, $RoR_A > RoR_B$ (indicated in blue) and $RoR_A > \frac{b}{2}$ (indicated in yellow) for different values of $\alpha$, keeping $v = 0.05$ and $\beta = 0.95$ fixed.
The blue region shows that there is a clear threshold of $b$ and $p$ beyond which $RoR_A > RoR_B$.
This is explained by the increasing nature of the revenue gap $(RoR_A - RoR_B)$ with $b$ and $p$.
But more interestingly, the threshold value of $b$ and $p$ decreases as $\alpha$ is increased toward $e$.
Whereas the yellow region shows that if the fraction of forward-looking customers is not too small, the firm should choose to run a reward program most of the time except for when $b$ is large; larger $b$ values mean that customers make more exogenous visits, so a reward program is no longer needed to entice visits, but only decreases the profits of the reward program merchant.
The intersection of two regions, \ie, the region in green, indicates that the range of values of $b$ for which the reward program is strictly profitable increases as $p$ increases.
We formally show this result next.

\begin{figure*}[t!]
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{\figurefolder/bp_pair_both_al0p5.png}
\caption{$\alpha = 0.5$}
\end{subfigure}%
~ 
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{\figurefolder/bp_pair_both_al1p0.png}
\caption{$\alpha = 1$}
\end{subfigure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{\figurefolder/bp_pair_both_al2p0.png}
\caption{$\alpha = 2$}
\end{subfigure}%
~ 
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{\figurefolder/bp_pair_both_al2p5.png}
\caption{$\alpha = 2.5$}
\end{subfigure}

\caption{{Regions where $RoR_A > RoR_B$ (blue), where $RoR_A > \frac{b}{2}$ (yellow) and where both are true (green) for different values of $\alpha$. In all cases, $\beta = 0.95$ and $v = 0.05$.}}
\label{fig:offer_reward_or_not}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[scale = 0.6]{\figurefolder/b_region_v2.png}
\caption{The upper and lower bounds on $b$ as a function of $p$. Here $v = 0.05$ and $\alpha \rightarrow e$.}
\label{fig:b_region}
\end{figure*}

For any fixed $\alpha$, the exact conditions on $p$, $b$ and $v$ for $RoR_A > RoR_B$ and $RoR_A > \frac{b}{2}$ are rather complex. We will first focus on one particular simple case: $\alpha \rightarrow e$. 

\begin{lemma}
As $\alpha \rightarrow e$, $RoR_A > RoR_B$ if and only if the following condition on $b$ holds:
\begin{equation}
b > 2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)}
\end{equation}
\end{lemma} 
\proof
First we compute the following quantity.
\begin{equation*}
\lim_{\alpha \to e} \frac{e-\alpha}{b\alpha}\log{1+\frac{b\alpha}{e-\alpha}}
\end{equation*}
Let $\frac{e-\alpha}{b\alpha} = x$, then it is easy to see that the above limit is equivalent to $\lim_{x\to \infty} \frac{\log(1+x)}{x} = 0$. Then as $\alpha \rightarrow e$, we have the following expressions for $RoR_A$ and $RoR_B$.
\begin{equation*}
RoR_A = (1-ev)\left(p+(1-p)\frac{b}{2} \right)
\end{equation*}
\begin{equation*}
RoR_B = (1-v)(1-p)\left(1-\frac{b}{2} \right)
\end{equation*}
And our condition $RoR_A > RoR_B$ simplifies.
\begin{align*}
(1-ev)\left(p+(1-p)\frac{b}{2} \right) &> (1-v)(1-p)\left(1-\frac{b}{2} \right) \\
\frac{b}{2}(1-p)(1-ev+1-v) &> (1-v)(1-p)-(1-ev)p \\
b &> 2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)} 
\end{align*}

\endproof

The above lemma gives a lower bound on $b$ for $RoR_A > RoR_B$ in terms of $p$ and $v$. In order for the reward program to be strictly better than the traditional pricing model, we also need $RoR_A > \frac{b}{2}$. The following lemma shows that this condition gives a corresponding upper bound on $b$.

\begin{lemma}
As $\alpha \rightarrow e$, $RoR_A > \frac{b}{2}$ if and only if the following condition on $b$ holds:
\begin{equation}
b < \frac{2p}{p+\frac{ev}{1-ev}}
\end{equation}
\end{lemma} 

\proof
The condition $RoR_A > \frac{b}{2}$ is equivalent to:
\begin{align*}
(1-\alpha v)\left(p \frac{e}{\alpha}\left(1-\frac{e-\alpha}{b\alpha}\log \left(1+\frac{b\alpha}{e-\alpha} \right) \right)+(1-p)\frac{b}{2}\right) &> \frac{b}{2} \\
\frac{e}{\alpha} \left(1-\frac{e-\alpha}{b\alpha}\log \left(1+\frac{b\alpha}{e-\alpha} \right) \right) &> \frac{b}{2p}\left(\frac{1}{1-\alpha v}-(1-p) \right)
\end{align*}
As $\alpha \rightarrow e$, the left term above approaches 1 and we are left with:
\begin{align*}
b &< \frac{2 p (1-ev)}{1-(1-p)(1-ev)} \\
&= \frac{2p(1-ev)}{p-pev+ev} \\
&= \frac{2p}{p+\frac{ev}{1-ev}}
\end{align*}
\endproof

We combine the above two lemmas to get an intuitive necessary and sufficient condition on $p$ for the reward program to be ``strictly better''. 

\begin{lemma}
As $\alpha \rightarrow e$, for the reward program to be strictly better, a necessary and sufficient condition on $p$ is:
\beq
\label{eq:necp}
p > 1 - \frac{1-ev}{1-ev^2}
\eeq
\end{lemma}
%\proof
%\endproof

Thus, for any choice of $v$, and $p$ obeying the above condition, the combination of the above lemmas gives an interval of $b$ values for which the reward program is the most profitable choice for the merchant. 
Figure~\ref{fig:b_region} shows the bounds on $b$ for varying values of $p$, keeping $v = 0.05$ fixed, and restricting the range of $b$ values in $[0,1]$. 
Notice that the upper bound on $b$ increases as a function of $p$ while the lower bound decreases with $p$, so the interval of $b$ values where the reward program is strictly better increases with $p$. 
We formalize this observation in the next lemma. 

\begin{lemma}
As $\alpha \rightarrow e$ and $p$ obeying Eq.~\ref{eq:necp}, as $p$ increases, the range of values of $b$ for which the reward program is strictly better increases.
\end{lemma}
%\proof
%\endproof

Figure~\ref{fig:b_restrictions} shows the upper and lower bounds on $b$ for all valid pairs of $p$ and $v$ with $\alpha \rightarrow e$. 
The top plot shows the lower bound on $b$ and the bottom plot depicts the upper bound. 
For a particular $(p,v)$ pair, if the color on the top plot is darker than the corresponding color on the bottom plot, then this pair has a valid $b$ interval in which the reward program is strictly better. 
This figure also exhibits the increasing range of $b$ values with increasing $p$; for large values of $p$ and moderate values of $v$, we observe no restrictions on $b$ for the reward program to be strictly better. 
We combine all the above observations into the following theorem.

\begin{figure}[h!]
\begin{centering}
\includegraphics[scale = 0.75]{\figurefolder/b_bounds.png}
\caption{Bounds on $b$ for various values of $p$ and $v$ at $\alpha \rightarrow e$. Top shows lower bounds on $b$ for $RoR_A \geq RoR_B$ and bottom shows upper bounds of $b$ for $RoR_A \geq \frac{b}{2}$.}
\label{fig:b_restrictions}
\end{centering}
\end{figure}

\begin{theorem}
Under proportional budgeting, as $\alpha\rightarrow e$, a necessary and sufficient condition for reward program to be strictly better is a lowerbound on $p$ which increases with $v$.  
And as $p$ increases beyond the lowerbound, the region of allowable $b$ for which the reward program is strictly better becomes larger. 
\end{theorem}
%\proof
%\endproof

Now we generalize the above result for all values of $\alpha$. The conditions are more complex but the results and intuitions are similar. 

\begin{lemma}
Fix $\alpha \in (0, e)$. For any $(p,v)$ pair, there exists some upper bound $b_1 \in [0,1]$ such that for all $b \leq b_1$, $RoR_A \geq \frac{b}{2}$.
\end{lemma}

\proof
We delay the proof of this lemma to first prove a helpful proposition. It is a straightforward computation to see that the condition of $RoR_A \geq \frac{b}{2}$ is equivalent to:
\begin{gather*}
\frac{1}{b}\left(1-\frac{e-\alpha}{b\alpha}\log \left(1+\frac{b\alpha}{e-\alpha} \right) \right) \geq \frac{\alpha(1-(1-p)(1-\alpha v))}{2pe(1-\alpha v)} \\
\iff
g(b; \alpha) \geq h(p, v; \alpha)
\end{gather*}
where we have defined functions $g(b)$ and $h(p,v)$ for fixed $\alpha$ for the above inequalities. 

\begin{proposition}
For a fixed $\alpha$, $g(b)$ is non-increasing for all $b \in [0,1]$. 
\end{proposition}

\proof
We take the derivative of $g$:
\begin{align*}
g'(b) &= \frac{2(e-\alpha)}{b^3 \alpha} \log\left(1+\frac{b\alpha}{e-\alpha} \right) - \frac{1}{b^2}-\frac{1}{b^2\left(1+\frac{b\alpha}{e-\alpha}\right)} \leq 0 \\
&\iff \frac{2(e-\alpha)}{b \alpha} \log\left(1+\frac{b\alpha}{e-\alpha} \right) \leq 1+\frac{1}{1+\frac{b\alpha}{e-\alpha}} \\
&\iff \frac{2\log(1+x)}{x} \leq 1+\frac{1}{1+x}
\end{align*}
where $x = \frac{b\alpha}{e-\alpha}$, and as $b \in [0,1]$, $x \in [0, \frac{\alpha}{e-\alpha}]$. We can see that as $x \rightarrow 0$, the above inequality is an equality. 
We represent the LHS of the above equation as $L(x)$ and RHS as $R(x)$. Next we show that $L(x)$ increses slowly as compared to $R(x)$ thereby proving the proposition.
First show that in the range of $x$ the following holds true:

\beq
\label{eq:eq00}
\left(2-\frac{1}{1+x}\right)^2 \le 2\log(1+x) + 1
\eeq
To show the above observe that at $x\rightarrow 0$ both the LHS and RHS are equal. And it is easy to show that the derivative of LHS is lower than the derivative of RHS for all $x\ge 0$ as shown.
\begin{align*}
& (1+x) + \frac{1}{1+x} \ge 2\\
\implies & 2 - \frac{1}{1+x} \le 1 + x\\
\implies & (2-\frac{1}{1+x})\cdot \frac{1}{1+x} \le 1\\
\implies & 2\cdot(2-\frac{1}{1+x})\cdot (\frac{1}{1+x})^2 \le \frac{2}{1+x}
\end{align*}
The left hand side is the derivative of the above LHS and right hand side is the derivative of the above RHS.

Now we can rearrange Eq.~\ref{eq:eq00} as follows:
\begin{align*}
& \left(2-\frac{1}{1+x}\right)^2 \le 2\log(1+x) + 1\\
\implies & (1 + \frac{x}{1+x})^2 \le 2\log(1+x) + 1\\
\implies & (\frac{x}{1+x})^2 + \frac{2x}{1+x} \le 2\log(1+x)\\
\implies & 2\left(\frac{x}{1+x} - \log(1+x)\right) \le - (\frac{x}{1+x})^2\\
\implies & \frac{2\left(\frac{x}{1+x} - \log(1+x)\right)}{x^2} \le - (\frac{1}{1+x})^2
\end{align*}
The left hand side of above is $L'(x)$ and right hand side is $R'(x)$.

\endproof

Thus, $g(b)$ is decreasing in $b$, so for any $(p,v)$ pair, we may compute $h(p, v; \alpha)$, which will then fall into one of the following three cases.
\begin{itemize}
\item
$h(p,v;\alpha) \geq g(0)$. So no value of $b$ makes the reward program profitable.
\item
$h(p,v;\alpha) \leq g(1)$. So any value of $b$ makes the reward program profitable.
\item
$h(p,v;\alpha) = g(b_0)$ for some $b_0 \in (0,1)$. So the reward program is profitable for all $b \leq b_0$ and not otherwise.
\end{itemize}

The above proposition and discussion proves our lemma: for fixed $\alpha$ and any $(p,v)$ pair, there is some upperbound on $b$ s.t. $RoR_A > \frac{b}{2}$. 
\endproof

Now we again look at the conditions for $RoR_A > RoR_B$ to get a lower bound on $b$.  
\begin{lemma}
Fix $\alpha \in (0, e)$. For any $(p,v)$ pair, there exists some lower bound $b_0 \in [0,1]$ such that for all $b \geq b_0$, $RoR_A > RoR_B$.
\end{lemma}

\proof
Let $\frac{b\alpha}{e-\alpha} = X$. Then $RoR_A > RoR_B$ can be evaluated as follows:

\begin{eqnarray}
& p\frac{e}{\alpha}\left(1-\frac{\log(1+X)}{X}\right)(1-\alpha v + 1 - v) - p(1-v) + (1-p)\frac{b}{2}\left(1-\alpha v + 1-v\right) + p(1-v) > 1-v\notag\\
& \implies p\left(1 - \frac{\log(1+X)}{X}\right) + (1-p)\frac{b\alpha}{2e} > \frac{\alpha}{e} \cdot \frac{1-v}{1-\alpha v + 1 - v}\label{eq:ra>rb}
\end{eqnarray}

Since $\alpha$ is a constant, the LHS above is a function of $b$ and $p$. 
Let the LHS above be $L(b,p)$.
We first show that in the range of $b\in [0,1]$, $1 - \frac{\log(1+X)}{X} > \frac{b\alpha}{2e}$ which shows that $L(b,p)$ is increasing in $p$.

\begin{align*}
& 1-\frac{\log(1+X)}{X} > \frac{b\alpha}{2e}\\
\Leftrightarrow & X - \log(1+X) > \frac{b^2\alpha^2}{2e(e-\alpha)}
\end{align*}
Observe that LHS is equal to RHS when $b\rightarrow 0$. 
All we show is that LHS increases faster than RHS in the range of $b\in [0,1]$. 

\begin{align*}
\Leftrightarrow & \left(1 - \frac{1}{1+X}\right) \frac{\alpha}{e-\alpha} > \frac{b\alpha^2}{e(e-\alpha)}\\
\Leftrightarrow & \frac{1}{1+X} > \frac{e-\alpha}{e}\\
\Leftrightarrow & \frac{e}{e-\alpha} > 1 + \frac{b\alpha}{e-\alpha}
\end{align*}
And the last equation is true in the range of $b\in [0,1]$. Hence $L(b,p)$ increases with $p$.

Now we show that $L(b,p)$ increases with $b$ as well. First observe:

\beq
\frac{\partial L(b,p)}{\partial b} = p\left(\frac{\log(1+X) - \frac{X}{1+X}}{X^2}\right)\frac{\alpha}{e-\alpha} + (1-p)\frac{\alpha}{2e}
\notag
\eeq

Thus $\frac{\partial L(b,p)}{\partial b} > 0$ implies:

\begin{align*}
& (1-p)\frac{\alpha}{2e} > p\left(\frac{\frac{X}{1+X} - \log(1+X)}{X^2}\right)\frac{\alpha}{e-\alpha}\\
\Leftrightarrow & (1-p)\frac{b^2\alpha^2}{2e(e-\alpha)} > p \left(1 - \frac{1}{1+X} - \log(1+X)\right)
\end{align*}

Again the LHS and RHS are equal as $b\rightarrow 0$. All we show again is that LHS increases faster as compared to RHS.

\begin{align*}
\Leftrightarrow (1-p)\frac{b\alpha^2}{e(e-\alpha)} > p\left(\frac{1}{(1+X)^2} - \frac{1}{1+X} \right)\frac{\alpha}{e-\alpha}\\
\end{align*}
Clearly RHS is negative when $b\in (0,1]$ and LHS is positive. Hence proved.

Thus $L(b,p)$ is increasing in both $b$ and $p$. And the condition required is $L(b,p)$ is greater than some constant value which depends on $v$.
Hence for any $v$ there exists a smooth $(b_0,p_0)$ curve such that for all $b\ge b_0$ and $p\ge p_0$ revenue rate of reward program merchant is larger.

\endproof

We combine the above two lemmas as before to get the following theorem.

\begin{theorem}
Fix $\alpha \in (0, e)$. 
For any value of $v$, there exists a lowerbound $p_0$ such that for any $p$ greater than $p_0$, there exists a range $(b_0, b_1)$ between $0$ and $1$ such that for all $b$ lying between $b_0$ and $b_1$, offering the reward program is strictly better for $A$. 
\end{theorem}

The above results can be extremely helpful in the following way: if a merchant has good estimates of its target customer population, \ie, $b$ and $p$ values, it can find the appropriate reward budget ratios $\alpha$, which could make running a reward program strictly better against a traditional pricing competitor.
