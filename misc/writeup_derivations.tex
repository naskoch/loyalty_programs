\documentclass{article}
\usepackage{graphicx,fancyhdr,amsmath,amssymb,amsthm,subfig,url,hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\setlength\parindent{0pt}

\newtheorem{fact}{Fact}[section]
\newtheorem{rules}{Rule}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}[section]
%\newenvironment{proof}{{\bf Proof.  }}{\hfill$\Box$}
\newenvironment{proofof}[1]{\noindent {\em Proof of #1.  }}{\hfill$\Box$}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}{Claim}
\newtheorem{conjecture}{Conjecture}

\def \imp {\rightarrow}
\def \qed {\hfill $\Box$}
\def \R {{\mathbb R}}
\def \sR {{\mathcal R}}
\def \eps {\varepsilon}
\def \N {{\mathbb N}}
\def \Z {{\mathbb Z}}
\newcommand{\ignore}[1]{}
\newcommand{\fignore}[1]{#1}
\def \poly { \text{\rm poly~} }
\def \polylog { \text{\rm polylog~} }
\def\th{{\text{\rm th}}}

\begin{document}

\section{Problem description}
Consider a duopoly with two stores, $A$ and $B$, selling the same item. Store $A$ sells the item for a price $p$ while store $B$ sells it for price $p^{+} = p+\epsilon$ for some $\epsilon > 0$. Store $B$ also gives a reward $R$ to a customer after making $k$ purchases at $B$. Our goal is to find the optimal $k$ for store $B$ to maximize its rate of revenue under certain assumptions on customer behavior. \\

We assume that there is some exogenous probability, $\lambda$, during each purchase that forces the customer to go to store $B$. Let $0 < \beta \leq 1$ denote the discounting factor of future money. (NOTE/QUESTION: in the previous write-up, $\beta$ was called the expiry factor - is this still the correct interpretaion? If so, it seems like $\beta$ should be close to zero but in the discounting interpretation $\beta$ should be close to one) For simplicity, we assume the customer has the following utility as a function of price paid: $u(p) = v > 0$ and $u(p^+) = 0$. \\

We model the customer's decision problem as a dynamic problem. We index the number of visits the customer makes at store $B$ by $i$, for $0 \leq i \leq k-1$, and we refer a customer to be in state $i$ after having made $i$ visits to $B$. At state $i$, the customer has two possibilities:
\begin{enumerate}
\item
With probability $\lambda$, the customer must visit $B$, and she is now in state $i+1$.
\item
With probability $1-\lambda$, the customer may purchase from $A$ for utility $v$ and remain in state $i$ or purchase from $B$ for no utility but move to state $i+1$.
\end{enumerate}

Let $V(i)$ denote the long term expected reward at state $i$. Then we may model the decision problem as the following dynamic program.
\begin{gather*}
V(i) = \lambda \beta V(i+1) + (1-\lambda)\max\{v+\beta V(i),\beta V(i+1) \} \mbox{ for } 0\leq i \leq k-1 \\
V(k) = R
\end{gather*}

We will show that the decision process exhibits a phase transition; that is prior to some state $i_0$, the customer will only visit $B$ if she must do so exogenously but after $i_0$, she always decides to go to $B$. \\

Finally, we assume the customer has a look-ahead factor $t$, which models how many purchases ahead the customer looks ahead when making her current decision. This value will affect the phase transition of the decision process. Consider a distribution $T$ describing the look-ahead factor for consumers. We will focus on threshold distributions; for example, with probability $p$ the look-ahead is $t_1$ and with probability $1-p$, the look-ahead is $t_0$.

\section{Solving the DP and the phase transition}

NEED TO: find conditions under which $V(i)$ is increasing in $i$ and provide proof (these will be different than in the original write-up). The following lemma would be written as a part of this lemma. \\

\begin{lemma} We may write the DP as
\begin{equation*}
V(i) = \max\left\{ \frac{\lambda \beta V(i+1)+(1-\lambda)v}{1-(1-\lambda)\beta}, \beta V(i+1) \right\}
\end{equation*}
\end{lemma}

\begin{proof}
We have the following:
\begin{align*}
V(i) &= \lambda \beta V(i+1) + (1-\lambda)\max\{v +\beta V(i), \beta V(i+1) \} \\
&= \max\{\lambda \beta V(i+1) + (1-\lambda)(v+\beta V(i)), \beta V(i+1) \}
\end{align*}

Assuming $V(i)$ is the left term in the above maximum, we may solve the equation for that term.
\begin{gather*}
V(i) = \lambda \beta V(i+1) + (1-\lambda)(v+\beta V(i)) \\
(1-(1-\lambda)\beta) V(i) = \lambda \beta V(i+1) + (1-\lambda)v \\
V(i) = \frac{\lambda \beta V(i+1) + (1-\lambda)v}{1-(1-\lambda)\beta}
\end{gather*}\end{proof}

\begin{theorem} A phase transition occurs after the consumer makes $i_0$ visits to firm $B$, which evaluates to:
\begin{align*}
i_0 &= k - \left\lfloor{\log_{\beta}\left(\frac{v}{R\beta(1-\beta)} \right)}\right\rfloor \\
&\equiv k-\Delta
\end{align*}
\end{theorem}

(Note: we may need conditions as we did before - thing to check)

\begin{proof}
First we solve for the condition on $V(i+1)$ for us to choose firm $B$ over $A$ willingly.
\begin{gather*}
\beta V(i+1) \geq \frac{\lambda \beta V(i+1) + (1-\lambda)v}{1-(1-\lambda)\beta} \\
\iff \beta V(i+1) \left(1-\frac{\lambda}{1-(1-\lambda)\beta} \right) \geq \left(\frac{1-\lambda}{1-(1-\lambda)\beta} \right) v \\
\iff \beta V(i+1) \left(\frac{1-(1-\lambda)\beta -\lambda}{1-(1-\lambda)\beta} \right) \geq \left(\frac{1-\lambda}{1-(1-\lambda)\beta} \right) v \\
\iff \beta V(i+1) \left(\frac{(1-\lambda)(1-\beta)}{1-(1-\lambda)\beta} \right) \geq \left(\frac{1-\lambda}{1-(1-\lambda)\beta} \right) v \\
\iff \beta V(i+1) \geq \frac{v}{1-\beta} \\
\iff V(i+1) \geq \frac{v}{\beta(1-\beta)}
\end{gather*}
Let $i_0$ be the minimum state $i$ such that the above holds, so in particular $V(i_0) < \frac{v}{\beta(1-\beta)}$ but $V(i_0+1) \geq \frac{v}{\beta(1-\beta)}$. We know because $V$ is increasing in $i$ (still need to prove), this point is indeed a phase transition: $V(i) \geq \frac{v}{\beta(1-\beta)}$ for all $i > i_0$, so after this point, the consumer always chooses firm $B$. We may compute $V(i_0)$ easily using this fact.
\begin{equation*}
V(i_0) = \beta V(i_0+1) = \cdots = \beta^{k-i_0}V(k) = \beta^{k-i_0}R
\end{equation*}
Thus, we have the following:
\begin{gather*}
\beta^{k-i_0} < \frac{v}{R\beta(1-\beta)} \leq \beta^{k-(i_0+1)} \\ 
\iff k-i_0 < \log_{\beta}\left(\frac{v}{R\beta(1-\beta)} \right) \leq k-(i_0+1) \\
\iff i_0 > k - \log_{\beta}\left(\frac{v}{R\beta(1-\beta)} \right) \\
\iff i_0 = k - \left\lfloor \log_{\beta}\left(\frac{v}{R\beta(1-\beta)} \right) \right\rfloor \equiv k-\Delta
\end{gather*}
\end{proof}

\section{Look-ahead, threshold distribution}
Now we assume the look-ahead factor of a customer is drawn from some distribution $t \sim T$. The phase transition of the customer's DP will now depend on $t$.
\begin{equation*}
  i_0(t)=\begin{cases}
    i_0, & \text{if $t \geq \Delta$}.\\
    k-t, & \text{otherwise}.
  \end{cases}
\end{equation*}
Assuming a customer look-ahead distribution and a fixed reward size, $R$, we want to choose a $k$ to maximize the revenue per reward cycle. That is we want to maximize the quantity given by revenue of $B$ during the $k$ visits over the total number of purchases (at both $A$ and $B$) to reach $k$ visits at $B$. For simplicity we assume $p^+ = 1$, so the revenue of $B$ per reward cycle is $k-R$. The expected total number of purchases per reward cycle is $\frac{i_0(t)}{\lambda}+(k-i_0(t))$, where the first term represents the expected number of visits needed to reach the phase transition with exogonous visits to $B$ and the second term is just the remaining visits to $B$ to receive the reward. \\

Note that we can think of the length of the reward cycle as a random variable: length of cycle = $\tau+k-i_0(t)$ where $\tau$ is a random variable representing the number of visits needed to hit the phase transition and $E(\tau) = \frac{i_0(t)}{\lambda}$. Ideally, we would like to maximize the following objective function over $k$: $E_{\tau,t}\left(\frac{k-R}{\tau+k-i_0(t)} \right)$. We have some expressions for this objective function (which I can write up later or in separate document), but for now we will focus on a simpler objective: $E_t\left(\frac{k-R}{E_{\tau}(\tau+k-i_0(t)}\right) = E_t\left(\frac{k-R}{i_0(t)/\lambda+k-i_0(t)} \right)$. We hope that this change still models reality well (maybe we can prove this for further justification?). \\

In this section, we focus on a very simple threshold distribution given by the following.
\begin{equation*}
  i_0(t)=\begin{cases}
    t_1\geq \Delta, & \text{wp } p,\\
    t_0 < \Delta, & \text{wp } 1-p.
  \end{cases}
\end{equation*}

\begin{lemma}
The objective function for the above look-ahead distribution is given by:
\begin{equation*}
f(k) = \frac{\lambda (k-R)p}{k-(1-\lambda)\Delta}+\frac{\lambda (k-R)(1-p)}{k-(1-\lambda)t_0}
\end{equation*}
\end{lemma}

\begin{proof}
\begin{align*}
E_t\left(\frac{k-R}{\frac{i_0(t)}{\lambda}+k-i_0(t)} \right) &= \frac{(k-R)p}{\frac{i_0}{\lambda}+k-i_0}+\frac{(k-R)(1-p)}{\frac{k-t_0}{\lambda}+k-(k-t-0)} \\
&= \frac{\lambda(k-R)p}{i_0+\lambda(k-i_0)}+\frac{\lambda(k-R)(1-p)}{k-t_0+t_0\lambda} \\
&= \frac{\lambda(k-R)p}{k-\Delta+\lambda(\Delta)}+\frac{\lambda(k-R)(1-p)}{k-t_0+t_0\lambda} \\
&= \frac{\lambda (k-R)p}{k-(1-\lambda)\Delta}+\frac{\lambda (k-R)(1-p)}{k-(1-\lambda)t_0} \equiv f(k)
\end{align*}
\end{proof}

We wish to maximize this objective function for $k>\Delta$ (otherwise $i_0$ would be negative). Next, we will characterize the conditions under which we can maximize the function and what the maxima are.

\begin{lemma}
If $(1-\lambda)t_0 \leq R \leq (1-\lambda)\Delta$, the above objective function has real-valued critical points.
\end{lemma}

\begin{proof}
First we differentiate $f(k)$.
\begin{align*}
\frac{df}{dk} &= \frac{\lambda(k-(1-\lambda)\Delta)-\lambda(k-R)p}{(k-(1-\lambda)\Delta)^2}+\frac{\lambda(1-p)(k-(1-\lambda)t_0)-\lambda(k-R)(1-p)}{(k-(1-\lambda)t_0)^2} \\
&= \frac{\lambda p(R-(1-\lambda)\Delta)}{(k-(1-\lambda)\Delta)^2} + \frac{\lambda(1-p)(R-(1-\lambda)t_0)}{(k-(1-\lambda)t_0)^2}
\end{align*}
Setting equal to zero and solving for $k$ we get the following. Let $c_1 = R-(1-\lambda)\Delta$ and $c_2 = R-(1-\lambda)t_0$.
\begin{gather*}
pc_1(k-(1-\lambda)t_0)^2 = -(1-p)c_2(k-(1-\lambda)R)^2 \\
\iff (pc_1)k^2-(2pc_1(1-\lambda))k+(pc_1(1-\lambda)^2t_0^2 = (-(1-p)c_2)k^2+(2(1-p)(1-\lambda)c_2\Delta)k-((1-p)c_2(1-\lambda)^2\Delta^2) \\
\iff (pc_1+(1-p)c_2)k^2-2(1-\lambda)(pc_1t_0+(1-p)c_2\Delta)k+(1-\lambda)^2(pc_1t_0^2+(1-p)c_2\Delta^2) = 0
\end{gather*}
For the above to have real-valued solutions, we need:
\begin{gather*}
4(1-\lambda)^2(pc_1t_0+(1-p)c_2\Delta)^2 - 4(pc_1+(1-p)c_2)(1-\lambda)^2(pc_1t_0^2+(1-p)c_2\Delta^2) \geq 0 \\
\iff p^2c_1^2t_0^2+(1-p)^2c_2^2\Delta^2+2p(1-p)c_1c_2t_0\Delta - p^2c_1^2t_0^2+(1-p)^2c_2^2\Delta^2+p(1-p)c_1c_2t_0^2+p(1-p)c_1c_2\Delta^2 \geq 0 \\
\iff p(1-p)c_1c_2(2t_0\Delta-\Delta^2-t_0^2) \geq 0 \\
\iff -p(1-p)c_1c_2(t_0-\Delta)^2 \geq 0 \\
\iff -p(1-p)c_1c_2 \geq 0 \\
\iff c_1c_2 \leq 0
\end{gather*}
The constraint that $c_1c_2 \leq 0$ means $(R-(1-\lambda)\Delta)(R-(1-\lambda)t_0) \leq 0$. Because $R \geq 0$, $(1-\lambda) \geq 0$ and $t_0 < \Delta$, we also have that $(R-(1-\lambda)\Delta) < (R-(1-\lambda)t_0)$. So we must have:
\begin{gather*}
(R-(1-\lambda)\Delta) \leq 0 \leq (R-(1-\lambda)t_0) \\
\iff (1-\lambda)t_0 \leq R \leq (1-\lambda)\Delta
\end{gather*}
\end{proof}

Note that $\Delta$ depends on $R$, so the above inequality is more complicated than as written. TODO - see if we can get a nice inequality for $R$ not in terms of $\Delta$. (Can also add a sample plot here). \\

Now need to check that this is actually a maximum.

\section{Question that we need to answer}

Here is something that is concerning me. Look at the objective function again.
\begin{equation*}
f(k) = \frac{\lambda p (k-R)}{k-(1-\lambda)\Delta} + \frac{\lambda (1-p) (k-R)}{k-(1-\lambda)t_0}
\end{equation*}
We want to maximize this function over values of $k$ greater than $\Delta$. We know that $t_0 < \Delta$ so the denominator of the first term will always be less than that of the second term. Note even more that when $k$ is really close to $\Delta$ the first term dominates and will be really large. Obviously, when $k = (1-\lambda)\Delta < \Delta$, $f(k)$ will be infinite (as long as $R \neq \Delta$). And as $k$ increases from here, $f$ will decrease. I believe this objective function will always be maximized at $k=\Delta$ unless $R$ is also very close to $\Delta$ - which as we've seen on restrictions on $R$, may not be possible. We may need to tweek our objective function. \\

Some futher investigation on restrictions on $R$ to get a maximum from critical point. First we look at the second derivative of $f$.
\begin{equation*}
f''(k) = -\frac{2\lambda p (R-(1-\lambda)\Delta)}{(k-(1-\lambda)\Delta)^3}-\frac{2\lambda(1-p)(R-(1-\lambda)t_0)}{(k-(1-\lambda)t_0)^3}
\end{equation*}
We know the denominators will always be positive on our domain. We also know that for a critical point to be real we must have $(R-(1-\lambda)\Delta) \leq 0 \leq (R-(1-\lambda)t_0)$. So for a real-valued critical point to be a maximum, we must have:
\begin{gather*}
-\frac{2\lambda p (R-(1-\lambda)\Delta)}{(k-(1-\lambda)\Delta)^3}-\frac{2\lambda(1-p)(R-(1-\lambda)t_0)}{(k-(1-\lambda)t_0)^3} < 0 \\
\iff \frac{2\lambda p |R-(1-\lambda)\Delta|}{(k-(1-\lambda)\Delta)^3} < \frac{2\lambda (1-p)(R-(1-\lambda)t_0)}{(k-(1-\lambda)t_0)^3}
\end{gather*}
Again, the denominator of the first term will always be smaller than that of the second term, so will be difficult (more restrictions on $R$ and $\Delta$) for above to be true. \\

Furthermore, we need the critical point of $f$ to be greater than $\Delta$. The critical points of $f$ are given by:
\begin{equation*}
x = \frac{(1-\lambda)(pc_1t_0+(1-p)c_2\Delta) \pm (1-\lambda)|t_0-\Delta|(-p(1-p)c_1c_2)^{\frac{1}{2}}}{pc_1+(1-p)c_2}
\end{equation*}
And we need:
\begin{equation*}
\frac{(1-\lambda)(pc_1t_0+(1-p)c_2\Delta) + (1-\lambda)|t_0-\Delta|(-p(1-p)c_1c_2)^{\frac{1}{2}}}{pc_1+(1-p)c_2} \geq \Delta
\end{equation*}
Again, this gives more restrictions on $R$ and $\Delta$. I still need to solve for all these restrictions, but I wanted to write this up so far to let you know a concern.

\section{When should a store offer a reward?}

Notice the revenue rate function $f(k)$, fixing $\beta$, $t_0$, $p$, $R$ and $\lambda$, approaches $\lambda$ as $k \rightarrow \infty$. This limit makes sense, as $k$ approaching $\infty$ means that no reward will be given, so all visits to $B$ will be exogenous, with a revenue rate of $\lambda$. One question that we may find interesting is when it is impossible to beat the revenue rate of $\lambda$ (supposing $R$ is fixed) and thus not offer a reward at all. Even if fixing $R$ is not very realistic, understanding the objective function as a function of $\lambda$ may give us insight into what to set $R$. \\

Let's fix $\beta$, $p$, $R$ and let $t_0 = 0$ for now (for simplicity - can go in later and add more general case next). We wish to find the set of $\lambda \in [0,1]$ such that $f(k) \leq \lambda$ on $k \geq \Delta$: the $\lambda$'s for which we can not beat revenue rate of $\lambda$. I believe that there will always be some $\lambda_0$ such that for all $\lambda < \lambda_0$, we can increase the revenue rate by offering a reward and for all $\lambda \geq \lambda_0$, we cannot increase the revenue rate by offering a reward. (Note: I have not finished with the full proof of this yet, but going to write-up some ideas so far). \\

\begin{lemma} Fix $R > 0$, $\beta$, $p$, $v$ and $t_0 = 0$. Then $f(\Delta) \leq \lambda$ if and only if $\lambda \geq \frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}$. Note that when equality holds in the second inequality, it does in the first as well. \end{lemma}
\begin{proof}
We have:
\begin{align*}
f(\Delta) &= \frac{\lambda p (\Delta-R)}{\Delta-(1-\lambda)\Delta}+\frac{\lambda (1-p)(\Delta-R)}{\Delta} \\
&= \frac{p(\Delta-R)}{\Delta}+\frac{\lambda(1-p)(\Delta-R)}{\Delta} \\
&= \frac{(\Delta-R)(\lambda(1-p)+p)}{\Delta}
\end{align*}
Then:
\begin{gather*}
\frac{(\Delta-R)(\lambda(1-p)+p)}{\Delta} \leq \lambda \\
\iff \lambda-\frac{(\Delta-R)\lambda(1-p)}{\Delta} \geq \frac{p(\Delta-R)}{\Delta} \\
\iff \lambda \left(\frac{\Delta-(\Delta-R)(1-p)}{\Delta}\right) \geq \frac{p(\Delta-R)}{\Delta} \\
\iff \lambda \geq \frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}
\end{gather*}
Note for the last step above, we need $\Delta-(\Delta-R)(1-p) > 0$. But because $\Delta, R, (1-p) \geq 0$ then if $(\Delta-R) \leq 0$, the condition will always be satisfied (if $(\Delta-R) = 0$, $\Delta = R > 0$ and value is positive). And if $(\Delta-R) > 0$, $(1-p)(\Delta-R) \leq (\Delta-R) < \Delta$, so $\Delta-(\Delta-R)(1-p) > 0$.
\end{proof}

I believe the above threshold $\frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}$ is our desired $\lambda_0$. To show this we need to show that $f(\Delta)$ is a necessary and sufficient condition for $f(k)$ not exceeding $\lambda$ on $k \geq \Delta$. Clearly it is a necessary condition, so we just need to show it is sufficient. I believe the best way to do this is to show that for these $\lambda$, no critical points that are maxima exist in the range $[\Delta, \infty)$. If this is the case, then the max will occur at the the endpoints, which we have already argued will not be greater than $\lambda$. First we prove some lemmas about the roots of $f'$, assuming they are real-valued. \\

\begin{lemma}
If $p(R-(1-\lambda)\Delta)+(1-p)R > 0$, the objective function $f(k)$ (when $t_0 = 0$) has at least one real-valued critical point with value at least $(1-\lambda)\Delta$.
\end{lemma}
\begin{proof}
First note that we need $R-(1-\lambda)\Delta \leq 0 \leq R-(1-\lambda)t_0 = R$ for $f'$ to have real roots. The roots of $f'$ are given by:
\begin{equation*}
x = \frac{(1-\lambda)((1-p)R\Delta) \pm (1-\lambda)\Delta(-p(1-p)(R-(1-\lambda)\Delta)R)^{\frac{1}{2}}}{p(R-(1-\lambda)\Delta)+(1-p)R} \equiv C\pm D
\end{equation*}
We will show that $C = 
\frac{(1-\lambda)((1-p)R\Delta)}{p(R-(1-\lambda)\Delta)+(1-p)R} \geq (1-\lambda) \Delta$. Then $f'$ has at most one root less than $(1-\lambda)\Delta$; because if $C \geq (1-\lambda)\Delta$, for every $D$ it is not possible for both $C+D$ and $C-D$ to be less than $(1-\lambda)\Delta$. Note that $(1-\lambda)(1-p)R\Delta \geq 0$ and $p(R-(1-\lambda)\Delta) \leq 0$ and by assumption, the denominator of $C$ is positive as well. Therefore, we have:
\begin{align*}
C &= \frac{(1-\lambda)(1-p)R\Delta}{p(R-(1-\lambda)\Delta)+(1-p)R} \\
&\geq \frac{(1-\lambda)(1-p)R\Delta}{(1-p)R} = (1-\lambda)\Delta
\end{align*}
because $(1-p)R \geq (1-p)R+p(R-(1-\lambda)\Delta) > 0$.
\end{proof}

Note that the above lemma applies to any choice of parameters as long as that denominator is positive. Hopefully we can use it to understand the objective function in more general settings as well.

\begin{lemma} If $\lambda \geq \frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}$, the objective function $f(k)$ (when $t_0 = 0$) has at most one real-valued critical point greater than $\Delta$. \end{lemma}
\begin{proof}
Again note that we need $R-(1-\lambda)\Delta \leq 0 \leq R-(1-\lambda)t_0 = R$ for $f'$ to have real roots. The roots of $f'$ are given by:
\begin{equation*}
x = \frac{(1-\lambda)((1-p)R\Delta) \pm (1-\lambda)\Delta(-p(1-p)(R-(1-\lambda)\Delta)R)^{\frac{1}{2}}}{p(R-(1-\lambda)\Delta)+(1-p)R} \equiv C\pm D
\end{equation*}
We will show that $C = 
\frac{(1-\lambda)((1-p)R\Delta)}{p(R-(1-\lambda)\Delta)+(1-p)R} \leq \Delta$. Then $f'$ has at most one root great than $\Delta$; because if $C \leq \Delta$, for every $D$ it is not possible for both $C+D$ and $C-D$ to be greater than $\Delta$. Note that $(1-\lambda)(1-p)R\Delta \geq 0$ and $p(R-(1-\lambda)\Delta) \leq 0$. If $p(R-(1-\lambda)\Delta)+(1-p)R < 0$, then $C < 0 \leq \Delta$ and we are done. So we focus on the case that $p(R-(1-\lambda)\Delta)+(1-p)R > 0$. \\

We will think of $C$ as a function of $\lambda$.First we show that for $\lambda' = \frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}$, $C(\lambda') = \Delta$. It is a straightforward computation to see that $(1-\lambda') = \frac{R}{\Delta-(\Delta-R)(1-p)}$. And we have:
\begin{align*}
C(\lambda') &= \frac{(1-\lambda')(1-p)R\Delta}{p(R-(1-\lambda')\Delta)+(1-p)R} \\
&= \frac{(1-p)R^2\Delta}{\Delta-(\Delta-R)(1-p)} \cdot \frac{1}{p\left(R-\frac{R\Delta}{\Delta-(\Delta-R)(1-p)} \right)+(1-p)R} \\
&= \frac{(1-p)R^2\Delta}{\Delta-(\Delta-R)(1-p)} \cdot \frac{1}{\left(\frac{-pR(\Delta-R)(1-p)}{\Delta-(\Delta-R)(1-p)} \right)+(1-p)R} \\
&= \frac{(1-p)R^2\Delta}{-pR(\Delta-R)(1-p)+(1-p)R(\Delta-(\Delta-R)(1-p)))} \\
&= \frac{(1-p)R^2\Delta}{(1-p)R\Delta-(pR(\Delta-R)(1-p)+(1-p)R(\Delta-R)(1-p))} \\
&= \frac{(1-p)R^2\Delta}{(1-p)R\Delta-(1-p)R(\Delta-R)} = \frac{(1-p)R^2\Delta}{(1-p)R^2} = \Delta
\end{align*}
Next notice that when the denominator is positive, $C$ is a decreasing function in $\lambda < 1$; increasing $\lambda$ decreases the value of the nominator and increases the value of the denominator (the first term becomes less negative so the whole thing becomes more positive). So by these two facts, $C(\lambda) \leq \Delta$ for $\lambda \geq \lambda'$ as defined above. \\

Finally, we need that the denominator is non-zero. So we need $R \neq p(1-\lambda)\Delta$. But we already have for real roots that $R \geq (1-\lambda)\Delta$. So for $p < 1$, we have a non-zero denominator.
\end{proof}

Now we must prove that $f(\Delta) \leq \lambda$ is a sufficient condition the maximum of $f$ on $k\geq \Delta$ being no greater than $\lambda$.

\begin{lemma}
Fix $R > 0$, $\beta$, $p$, $v$ and $t_0 = 0$. If $f(\Delta) \leq \lambda$, then $f(k) \leq \lambda$, $\forall k \geq \Delta$. 
\end{lemma}

\begin{proof}
We have that $f(\Delta) \leq \lambda$ and $f(k) \rightarrow \lambda$ as $k \rightarrow \infty$. Then if we show that no strict maximum occurs on the interval $(\Delta, \infty)$, we have shown the claim. We will break the proof down in cases based on the sign on the derivative of $f$ at delta. Note that because $f(\lambda) \leq \Delta$, we have a $\lambda$ such that our previous lemma applies. \\

\textbf{Case 1:} $(f'(\Delta) = 0)$ For the derivative of $f$ at $\Delta$ to be zero, no critical points of $f$ may occur after $\Delta$. To see this, we make use of our previous lemma; if $\Delta$ is the larger of the two roots of $f'$ then we are done, and by the previous result, if it is the smaller of the two roots, it must be the case that both roots are actually $\Delta$. Because no critical points occur after $\Delta$, there are no strict maximum in the interval $(\Delta, \infty)$. \\

\textbf{Case 2:} $(f'(\Delta) < 0)$ By our previous lemma, only one critical point may occur after $\Delta$, so by the negative sign of $f'(\Delta)$, that critical point may only be a minimum. \\

\textbf{Case 3:} $(f'(\Delta) > 0)$ (NOT COMPLETED YET, so here are some notes) Here we want to show that in fact no critical points occur after $\Delta$, so no strict maximum may occur on the interval of interest. I am still working on this proof - I think it won't take too much longer. \\

Here I prove that the max root of $f'(k) < \Delta(1+\lambda)$. We have by the sign of the derivative:
\begin{equation*}
f'(\Delta) = \frac{p(R-(1-\lambda)\Delta)+\lambda^2(1-p)R}{\lambda^2\Delta} > 0
\end{equation*}
We know the denominator is positive (for $\Delta > 0$: need this as a condition and if $\Delta = 0$, program adoption never happens, so we should not use a program). For real roots of $f'$, $(R-(1-\lambda)\Delta) \leq 0$, so the above states $\lambda^2(1-p)R > |p(R-(1-\lambda)\Delta|$. Then the max root of $f'$ is:
\begin{align*}
x &= \frac{(1-\lambda)(1-p)R\Delta+(1-\lambda)\Delta(-p(1-p)(R-(1-\lambda)\Delta)R)^{1/2}}{p(R-(1-\lambda)\Delta)+(1-p)R} \\
&< \frac{(1-\lambda)(1-p)R\Delta+(1-\lambda)\Delta((1-p)R\lambda^2(1-p)R)^{1/2}}{p(R-(1-\lambda)\Delta)+(1-p)R} \\
&= \frac{(1-\lambda)(1-p)R\Delta+(1-\lambda)\Delta(1-p)\lambda R}{p(R-(1-\lambda)\Delta)+(1-p)R} \\
&= \frac{(1-\lambda)(1-p)R\Delta(1+\lambda)}{p(R-(1-\lambda)\Delta)+(1-p)R} \\
&\leq \frac{(1-p)R^2\Delta(1+\lambda)}{(1-p)R^2} \\
&= (1+\lambda)\Delta
\end{align*}
Where in the second line above we have used $\lambda^2(1-p)R > |p(R-(1-\lambda)\Delta|$ and in the second to last line we have used the facts that $\lambda \geq \frac{p(\Delta-R)}{\Delta-(\Delta-R)(1-p)}$ and $x$ is non-increasing in $\lambda$ (and done the same exact computations as in the previous lemma).
\end{proof}
\end{document}