\subsection{Merchant Objective Dynamics}

\subsubsection{Optimizing Reward Parameters}

So far we have characterized the customer behavior within the duopoly without concern about the particular reward design parameters. 
In this section, we derive optimal parameters for the reward program design with the objective of maximizing the revenue of the reward program merchant. 
Interestingly, we see that maximizing revenue corresponds to minimizing the influence zone, as illustrated above.

\begin{theorem}
Under proportional promotion budgeting, the optimal reward distance ($k$) and the optimal budget proportion ($\alpha$) for merchat $A$ follow the relation $\alpha k = \frac{e}{(1-\beta)}$ for all distributions of $\lambda$ as long as $\beta$ is close to 1.
\end{theorem}
\proof
Let $\theta = \frac{\Delta}{k}$. First, we evaluate $RoR_A$.
We substitute the value of the phase transition point obtained above in the rate of revenue equation for $A$ to reevaluate it. 
And since we assume that $\lambda$ and $t$ are drawn independent of each other, we can separate the expectation terms and evaluate them sequentially, first over $t$, then over $\lambda$.
\begin{align*}
RoR_A =& \underset{\lambda, t}E\left[\frac{k-R}{i_0(t)/\lambda + k - i_0(t)}\right]\\
                                       =& \underset{\lambda}E\left[p\cdot\frac{k-R}{i_0/\lambda + k - i_0} + (1-p)\frac{\lambda(k-R)}{k}\right]\\
                                       =& \underset{\lambda}E\left[p\cdot\frac{\lambda(k-R)}{k\lambda + i_0(1-\lambda)} + (1-p)\frac{\lambda(k-R)}{k}\right]\\
                                       =& \underset{\lambda}E\left[p\cdot\frac{\lambda(1-\alpha v)}{1-\theta(1-\lambda)} + (1-p)\lambda (1-\alpha v)\right]\\
\end{align*}

Observe that the term inside the expectation is maximized when $\theta$ is maximized for all values of $\lambda \in (0,1)$.
Using Leibniz' Rule, we can conclude that the integral itself is maximized when $\theta$ is maximized, which as shown above, is equivalent to minimizing the influence zone.
As shown in Remark~\ref{rem:inf_zone}, this happens at $\alpha k = \frac{e}{1-\beta}$. 
And at this point, $\theta = \frac{\Delta}{k} = \frac{\alpha}{e}$.
\endproof

An interesting point to observe above is that if $\alpha$ is constant, then maximizing the revenue objective is equivalent to minimizing the influence zone.
This result matches the following intuition - the faster the merchant can get customers to adopt the reward program, the more purchases they will make with the merchant in the long run - but is stronger as it actually maximizes the revenue objective as well. 
Although, reward point accelerations are common and effective mechanisms to get customers to adopt reward programs, we have shown that designing the reward program so that a minimum number of such accelerations is required leads to maximizing merchant's revenue. 
The condition that $\beta$ be close to 1 is not very restrictive, as the discount factor is expected to be high in most cases. 
Note that because $k \geq \Delta$, the above also shows $\alpha \leq e$. 
Finally, observe that we need $R > \frac{(1-\lambda)v}{1-\beta}$ for $V$ to be increasing. 
We meet this condition with proportional budgeting when $k = \frac{e}{\alpha(1-\beta)}$ as $R = \alpha k v = \frac{e v}{1-\beta} \geq \frac{v}{1-\beta} \geq \frac{(1-\lambda)v}{1-\beta}$. 

The above framework can be used for optimizing for the reward parameters to maximize $A$'s rate of revenue, for varying distributions of the customer population.
That is, if a merchant has a good estimate of its customer population's distribution, it can easily utilize the above theorem to optimize its reward scheme.
We leave the competitive study where merchant $B$ could strategize on its discount value $v$ for future work.

Figure~\ref{fig:alpha_max} shows how the rate of revenue of the reward program merchant varies with $\alpha$, after fixing $\alpha k$ as in our previous theorem, for various distributions of the loyalty bias parameter. 
We observe three general patterns for $RoR_A$: for large values of $v$, $RoR_A$ decreases along all feasible values of $\alpha$; for small values of $v$, it increases for all values of $\alpha$; and for some values of $v$ in between, it is convex with a minimizing $\alpha$ in $(0, e)$. 
That is, the rate of revenue for $A$ is maximized at $\alpha$ being $0$ or $e$, and no maximizer exists between $0$ and $e$ across distributions.
We believe this to be true for all important distributions, similar to what our simulations suggest.
Thus, the reward program merchant only needs to decide between not offering the reward (setting $\alpha$ as $0$) or offering the highest feasible reward (setting $\alpha$ to $e$) in our model.
Note that the exact values of $v$ for which these patterns occur also depend on $p$ and the parameters of the specific distribution for loyalty bias. 
In the following subsection, we explore these conditions in detail for the uniform distribution of loyalty bias for fixed $\alpha$. 

\begin{figure*}[t!]
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/rora_alpha_unif_v2.png}
\caption{Uniform distribution.}
\end{subfigure}%
~ 
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/rora_alpha_normal_v2.png}
\caption{Normal distribution.}
\end{subfigure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/rora_alpha_logit_v2.png}
\caption{Logit-normal distribution.}
\end{subfigure}

\caption{{Rate of revenue for reward merchant as a function of $\alpha$ (with $k = \frac{e}{\alpha(1-\beta)}$) for different distributions. For all distributions, $\beta = 0.9$, $p = 0.9$ and $v$ varies as labeled. The uniform distribution is on $(0,b]$ with $b = 0.9$; the normal distribution has $\mu = 0.5$ and $\sigma = 0.1$; and the logit-normal distribution is the standard on $[0,1]$.}}
\label{fig:alpha_max}
\end{figure*}

\subsubsection{Revenue Comparisons}

We characterize the conditions for when it is strictly better for $A$ to offer a reward program for a specific distribution of the loyalty bias parameter - when $\lambda$ for every customer is drawn uniformly at random between $(0,b]$ where $b$ is less than $1$. We will assume this distribution for the remainder of the section.
This condition boils down to two situations: first, the rate of revenue for $A$ should be higher than that of $B$ and second, that the rate of revenue for $A$ should be higher than it could have achieved by not offering the reward program at the same fixed price.
First, we evaluate the expected rates of revenue for both $A$ and $B$ under the optimality relation between $k$ and $\alpha$ mentioned above with $\lambda$ being drawn from a uniform distribution.

\begin{align*}
RoR_A =& \underset{\lambda}E\left[p\cdot\frac{\lambda(1-\alpha v)}{1-\theta(1-\lambda)} + (1-p)\lambda (1-\alpha v)\right]\\
      =& pk\cdot\frac{1-\alpha v}{\Delta}\cdot\left(1 - \frac{k-\Delta}{b\Delta}\log\left(1+\frac{b\Delta}{k-\Delta}\right)\right) + (1-p)\frac{bk(1-\alpha v)}{2k}\\
      =& (1-\alpha v) \left(p\frac{e}{\alpha}\left(1-\frac{e-\alpha}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right)\right) + (1-p)\frac{b}{2}\right)
\end{align*}

\begin{align*}
RoR_B =& \underset{\lambda, t}E\left[\frac{(i_0(t)\lambda - i_0(t))(1-v)}{i_0(t)/\lambda + k - i_0(t)}\right]\\
      =& \underset{\lambda}E\left[p\cdot\frac{(i_0/\lambda - i_0)(1-v)}{i_0/\lambda + k - i_0} + (1-p)\frac{(k/\lambda - k)(1-v)}{k/\lambda}\right]\\
      =& \underset{\lambda}E\left[p\cdot\frac{i_0(1-\lambda)(1-v)}{k\lambda + i_0(1-\lambda)} + (1-p)(1-\lambda)(1-v)\right]\\
      =& p\cdot\frac{i_0(1-v)}{b(k-i_0)^2}\left(k\log\left(1+\frac{b(k-i_0)}{i_0}\right) - b(k-i_0)\right) + (1-p)(1-\frac{b}{2})(1-v)\\
      =& p\cdot\frac{i_0(1-v)}{k-i_0}\left(\frac{k}{b(k-i_0)}\log\left(1+\frac{b(k-i_0)}{i_0}\right) - 1\right) + (1-p)(1-\frac{b}{2})(1-v)\\
      =& (1-v)\left(p\cdot\frac{e-\alpha}{\alpha}\left(\frac{e}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right) - 1\right) + (1-p)(1-\frac{b}{2})\right)\\
      =& (1-v)\left(p\frac{e}{\alpha}\left(\frac{e-\alpha}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right) - \frac{e-\alpha}{e}\right) + (1-p)(1-\frac{b}{2})\right)
\end{align*}

\begin{comment}
Under approximation get:

\begin{align*}
RoR_A \sim & \frac{b}{2}(1-\alpha v)\left(\frac{pe}{e-\alpha} + (1-p)\right)\\
      \sim & \frac{b}{2}(1-\alpha v) \left(1 + \frac{p\alpha}{e-\alpha}\right)\\
       \sim & \frac{b}{2}\left(1-\alpha v + \frac{p\alpha}{e-\alpha} - \frac{pv\alpha^2}{e-\alpha}\right)
\end{align*}

Maximizing above gives:

\begin{align*}
& -v + \frac{ep}{(e-\alpha)^2} - \frac{(e-\alpha)2pv\alpha + pv\alpha^2}{(e-\alpha)^2}\\
&= -v + \frac{ep}{(e-\alpha)^2} - \frac{pv\alpha(2e-\alpha)}{(e-\alpha)^2} = 0\\
&\implies ep - 2pe\alpha v + pv\alpha^2 = v(e^2 - 2e\alpha + \alpha^2)\\
&\implies e(p-ev) = (\alpha^2 v - 2e\alpha v)(1-p)\\
&\implies \alpha^2 - 2e\alpha = \frac{e(p-ev)}{v(1-p)}\\
&\implies (e-\alpha)^2 = \frac{ep - e^2v + e^2 v - e^2 vp}{v(1-p)}\\
&\implies (e-\alpha)^2 = \frac{ep(1-ev)}{v(1-p)}\\
&\implies \alpha = e - \sqrt{\frac{ep(1-ev)}{v(1-p)}}
\end{align*}
\end{comment}

Observe that both the above equations have a left term and a right term. The left term is the rate of revenue obtained from strategic customers whereas the right term is that obtained from the myopic customers.
As $\alpha$ ranges between $0$ and $e$, the value on the left term increases from $0$ for $RoR_A$ and decreases to $0$ for $RoR_B$.
That is, by controlling the reward budget ratio, $A$ is able to gain the entire strategic customer base.
But observe how $RoR_A$ varies with $\alpha$:
the marginal revenue term $(1-\alpha v)$ decreases with $\alpha$ as the merchant gives higher rewards to customers, but the market share term increases as $A$ gains more strategic customer base. 
As $\alpha \to 0$, $RoR_A \to b/2$, \ie, the revenue earned is only due to the loyalty bias, and is equivalent to the reveue earned by $A$ when not running any reward program.

Figure~\ref{fig:offer_reward_or_not} illustrates the region in terms of the customer parameters $(b,p)$ where it is better for $A$ to offer a reward program, \ie, $RoR_A > RoR_B$ (indicated in blue) and $RoR_A > \frac{b}{2}$ (indicated in yellow) for different values of $\alpha$, keeping $v = 0.05$ and $\beta = 0.95$ fixed.
The blue region shows that there is a clear threshold of $b$ and $p$ values beyond which $RoR_A > RoR_B$.
But more interestingly, the threshold value of $b$ and $p$ decreases as $\alpha$ is increased toward $e$.
Whereas the yellow region shows that if the fraction of strategic customers is not too small, the firm should choose to run a reward program most of the time except for when $b$ is large; larger $b$ values mean that customers make more exogenous visits, so a reward program is no longer needed to entice visits, but only decreases the profits of the reward program merchant.
The intersection of two regions, \ie, the region in green, indicates that the range of values of $b$ for which the reward program is strictly profitable increases as $p$ increases.
We formally show this result next.

\begin{figure*}[t!]
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/bp_pair_both_al0p5.png}
\caption{$\alpha = 0.5$}
\end{subfigure}%
~ 
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/bp_pair_both_al1p0.png}
\caption{$\alpha = 1$}
\end{subfigure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/bp_pair_both_al2p0.png}
\caption{$\alpha = 2$}
\end{subfigure}%
~ 
\begin{subfigure}[t]{0.5\textwidth}
\centering
\includegraphics[height=2in]{./figures/bp_pair_both_al2p5.png}
\caption{$\alpha = 2.5$}
\end{subfigure}

\caption{{Regions where $RoR_A > RoR_B$ (blue), where $RoR_A > \frac{b}{2}$ (yellow) and where both are true (green) for different values of $\alpha$. In all cases, $\beta = 0.95$, $v = 0.05$ and $\lambda$ drawn uniformly on $(0,b]$.}}
\label{fig:offer_reward_or_not}
\end{figure*}

For any fixed $\alpha$, the exact conditions on $p$, $b$ and $v$ for $RoR_A > RoR_B$ and $RoR_A > \frac{b}{2}$ are rather complex. We will first focus on one particular simple case: $\alpha \rightarrow e$. 

\begin{lemma}
As $\alpha \rightarrow e$, $RoR_A > RoR_B$ if and only if the following condition on $b$ holds:
\begin{equation}
b > 2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)}
\end{equation}
\end{lemma} 
\proof
First we compute the following quantity.
\begin{equation*}
\lim_{\alpha \to e} \frac{e-\alpha}{b\alpha}\log\left(1+\frac{b\alpha}{e-\alpha}\right)
\end{equation*}
Let $\frac{e-\alpha}{b\alpha} = x$, then it is easy to see that the above limit is equivalent to $\lim_{x\to \infty} \frac{\log(1+x)}{x} = 0$. Then as $\alpha \rightarrow e$, we have the following expressions for $RoR_A$ and $RoR_B$.
\begin{equation*}
RoR_A = (1-ev)\left(p+(1-p)\frac{b}{2} \right)
\end{equation*}
\begin{equation*}
RoR_B = (1-v)(1-p)\left(1-\frac{b}{2} \right)
\end{equation*}
And our condition $RoR_A > RoR_B$ simplifies.
\begin{align*}
(1-ev)\left(p+(1-p)\frac{b}{2} \right) &> (1-v)(1-p)\left(1-\frac{b}{2} \right) \\
\frac{b}{2}(1-p)(1-ev+1-v) &> (1-v)(1-p)-(1-ev)p \\
b &> 2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)} 
\end{align*}

\endproof

The above lemma gives a lower bound on $b$ for $RoR_A > RoR_B$ in terms of $p$ and $v$. In order for the reward program to be strictly better than the traditional pricing model, we also need $RoR_A > \frac{b}{2}$. The following lemma shows that this condition gives a corresponding upper bound on $b$.

\begin{lemma}
As $\alpha \rightarrow e$, $RoR_A > \frac{b}{2}$ if and only if the following condition on $b$ holds:
\begin{equation}
b < \frac{2p}{p+\frac{ev}{1-ev}}
\end{equation}
\end{lemma} 

\proof
The condition $RoR_A > \frac{b}{2}$ is equivalent to:
\begin{align*}
(1-\alpha v)\left(p \frac{e}{\alpha}\left(1-\frac{e-\alpha}{b\alpha}\log \left(1+\frac{b\alpha}{e-\alpha} \right) \right)+(1-p)\frac{b}{2}\right) &> \frac{b}{2} \\
\frac{e}{\alpha} \left(1-\frac{e-\alpha}{b\alpha}\log \left(1+\frac{b\alpha}{e-\alpha} \right) \right) &> \frac{b}{2p}\left(\frac{1}{1-\alpha v}-(1-p) \right)
\end{align*}
As $\alpha \rightarrow e$, the left term above approaches 1 and we are left with:
\begin{align*}
b &< \frac{2 p (1-ev)}{1-(1-p)(1-ev)} \\
&= \frac{2p(1-ev)}{p-pev+ev} \\
&= \frac{2p}{p+\frac{ev}{1-ev}}
\end{align*}
\endproof

The previous two lemmas provide lower and upper bounds on $b$ for $RoR_A > RoR_B$ and $RoR_A > \frac{b}{2}$, respectively. For the reward program to be strictly better than all alternatives, both of these conditions must be met. We combine them to get an intuitive necessary and sufficient condition on $p$ for the reward program to be ``strictly better''. 

\begin{lemma}
As $\alpha \rightarrow e$, for the reward program to be strictly better on some values of $b$, a necessary and sufficient condition on $p$ is:
\beq
\label{eq:necp}
p > 1 - \frac{1-ev}{1-ev^2}
\eeq
\end{lemma}

\proof
 The values of $b$ for which both previous lemmas are met is given by:
\begin{align*}
2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)} < b < \frac{2p}{p+\frac{ev}{1-ev}}
\end{align*}
The above inequality is only valid when the lower bound is less than the upper bound. We may manipulate this inequality to get the simple condition on $p$ in our claim.
\begin{align*}
2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)} &< \frac{2p}{p+\frac{ev}{1-ev}} \\
\left(p+\frac{ev}{1-ev} \right)\left((1-v)+\frac{p}{1-p}(1-ev) \right) &<  p(1-v+1-ev) \\
(1-v)\frac{ev}{1-ev} &< p(1-ev)+\frac{p^2}{1-p}(1-ev) + \frac{p}{1-p}ev \\
(1-p)(1-v)\frac{ev}{1-ev} &< (1-p)p(1-ev)+p^2(1-ev)+pev \\
(1-v)\frac{ev}{1-ev} &< p\left(1+(1-v)\frac{ev}{1-ev} \right) \\
\frac{(1-v)ev}{(1-ev)+(1-v)ev} &< p \\
\frac{ev-ev^2}{1-ev^2} &< p \\
\frac{(1-ev^2)-(1-ev)}{1-ev^2} &< p \\
1-\frac{1-ev}{1-ev^2} &< p
\end{align*}
\endproof

\begin{figure*}[t]
\centering
\includegraphics[scale = 0.4]{./figures/b_region_v2.png}
\caption{The upper and lower bounds on $b$ as a function of $p$. Here $v = 0.05$ and $\alpha \rightarrow e$.}
\label{fig:b_region}
\end{figure*}

Thus, for any choice of $v$, and $p$ obeying the above condition, the combination of the above lemmas gives an interval of $b$ values for which the reward program is the most profitable choice for the merchant. 
Figure~\ref{fig:b_region} shows the bounds on $b$ for varying values of $p$, keeping $v = 0.05$ fixed, and restricting the range of $b$ values in $(0,1)$. 
Notice that the upper bound on $b$ increases as a function of $p$ while the lower bound decreases with $p$, so the interval of $b$ values where the reward program is strictly better increases with $p$. 
We formalize this observation in the next lemma. 

\begin{lemma}
As $\alpha \rightarrow e$ and $p$ obeying Eq.~\ref{eq:necp}, as $p$ increases, the range of values of $b$ for which the reward program is strictly better increases.
\end{lemma}

\proof
We know that the range of $b$ values in which we are interested is given by the interval.
\begin{align*}
2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)} < b < \frac{2p}{p+\frac{ev}{1-ev}}
\end{align*}
Because $p$ obeys eq.~\ref{eq:necp}, the above inequality is valid. We will show that the above upper bound increases with $p$ and the lower bound decreases with increasing $p$. Therefore, as $p$ increases, the interval of $b$ values for which the reward program grows. First consider the upper bound, $UB(p) = \frac{2p}{p+\frac{ev}{1-ev}}$.
\begin{align*}
UB'(p) = \frac{ev}{(1-ev)\left(p+\frac{ev}{1-ev} \right)^2} \geq 0, \mbox{ } \forall p
\end{align*}
Now we consider the lower bound, $LB(p) = 2\cdot \frac{(1-v) - \frac{p}{1-p}\cdot (1-ev)}{(1-v) + (1-ev)}$.
\begin{align*}
LB'(p) = -\frac{2(1-ev)}{(1-p)^2((1-v)+(1-ev))} \leq 0, \mbox{ } \forall p
\end{align*}
\endproof

Figure~\ref{fig:b_restrictions} shows the upper and lower bounds on $b$ for all valid pairs of $p$ and $v$ with $\alpha \rightarrow e$. 
The top plot shows the lower bound on $b$ and the bottom plot depicts the upper bound. 
For a particular $(p,v)$ pair, if the color on the top plot is darker than the corresponding color on the bottom plot, then this pair has a valid $b$ interval in which the reward program is strictly better. 
This figure also exhibits the increasing range of $b$ values with increasing $p$; for large values of $p$ and moderate values of $v$, we observe no restrictions on $b$ for the reward program to be strictly better. 
We combine all the above observations into the following theorem.

\begin{figure}[h!]
\begin{centering}
\includegraphics[scale = 0.55]{./figures/b_bounds.png}
\caption{Bounds on $b$ for various values of $p$ and $v$ at $\alpha \rightarrow e$. Top shows lower bounds on $b$ for $RoR_A \geq RoR_B$ and bottom shows upper bounds of $b$ for $RoR_A \geq \frac{b}{2}$.}
\label{fig:b_restrictions}
\end{centering}
\end{figure}

\begin{theorem}
Under proportional budgeting, as $\alpha\rightarrow e$, a necessary and sufficient condition for the reward program to be strictly better is a lowerbound on $p$ which increases with $v$.  
And as $p$ increases beyond the lowerbound, the region of allowable $b$ for which the reward program is strictly better becomes larger. 
\end{theorem}

Now we generalize the above result for all values of $\alpha$. The conditions are more complex but the results and intuitions are similar. The proofs are technical, and we leave them to the appendix. 

\begin{lemma}
\label{lem:lower_b}
Fix $\alpha \in (0, e)$. For any $(p,v)$ pair, there exists some upper bound $b_1 \in [0,1]$ such that for all $b \leq b_1$, $RoR_A \geq \frac{b}{2}$.
\end{lemma}


\begin{lemma}
\label{lem:upper_b}
Fix $\alpha \in (0, e)$. For any $(p,v)$ pair, there exists some lower bound $b_0 \in [0,1]$ such that for all $b \geq b_0$, $RoR_A > RoR_B$.
\end{lemma}


We combine the above two lemmas as before to get the following theorem.

\begin{theorem}
Fix $\alpha \in (0, e)$. 
For any value of $v$, there exists a lowerbound $p_0$ such that for any $p$ greater than $p_0$, there exists a range $(b_0, b_1)$ between $0$ and $1$ such that for all $b$ lying between $b_0$ and $b_1$, offering the reward program is strictly better for $A$. 
\end{theorem}

The above results can be extremely helpful in the following way: if a merchant estimates that the loyalty bias parameter is drawn from a uniform distribution and has good estimates of its target customer population, \ie, $b$ and $p$ values, it can find the appropriate reward budget ratios $\alpha$, which could make running a reward program strictly better against a traditional pricing competitor. 
More importantly, these results show that under mild assumptions on the customer poplation parameters, reward programs can be beneficial in the competitive duopoly model.
